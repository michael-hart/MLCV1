%% Initialisation

% Call init script in subfolder
init_rf2017;

% Set whether to plot results
plot_results = 1;

% Load the given toy spiral data
% Also allowed Toy_Gaussian, Toy_Circle, Caltech
[data_train, data_test] = getData('Toy_Spiral');

% Do a test plot to check the data is working
if (plot_results)
    plot_toydata(data_train);
%     scatter(data_test(:,1),data_test(:,2),'.b');
end

%% Bag the data

[N,D] = size(data_train);
frac = 1 - 1/exp(1); % Bootstrap sampling fraction: 1 - 1/e (63.2%)
[labels,~] = unique(data_train(:,end));

% Take four samples, display and save them, then continue
for i=1:4
    idx = randsample(N,ceil(N*frac),1);
    figure;
    plot_toydata(data_train(idx, :));
    % gcf: get current figure
    saveas(gcf, ['img/bagged_' num2str(i) '.png']);
end

save idx;

% A new training set for each tree is generated by random sampling from 
% dataset WITH replacement.
% idx = randsample(N,ceil(N*frac),1); 
prior = histc(data_train(idx,end),labels)/length(idx);

%% Split the first node and investigate

% Set the random forest parameters
param.num = 5;         % Number of trees
param.depth = 8;        % trees depth
param.splitNum = 50;     % Number of split functions to try
param.split = 'IG';     % Currently support 'information gain' only
param.split_func = 'quadratic';
clear idx;
load idx;
current_data = data_train(idx);


rootNode = struct('idx',[],'prob',[],'split_param', ...
                  struct('split_func', 'leaf'));
% Try a few different split functions and look at the results

[node, nodeL, nodeR] = splitNode(data_train, rootNode, param);
% nodeL contains the correct index list for IG testing
% ig = getIG(data_train, nodeL.idx);
              
trees = growTrees(data_train, param);

%%

% Set the random forest parameters
param.num = 5;         % Number of trees
param.depth = 8;        % trees depth
param.splitNum = 50;     % Number of split functions to try
param.split = 'IG';     % Currently support 'information gain' only
param.split_func = 'axis-aligned';
trees = growTrees(data_train, param);
%%
test_point = [-.5 -.7 1; .4 .3 1; -.7 .4 1; .5 -.5 1];
for n=1:length(test_point)
    leaves = testTrees([test_point(n,1:2) 0],trees);

    % average the class distributions of leaf nodes of all trees
    p_rf = trees(1).prob(leaves,:);
    p_rf_sum = sum(p_rf)/length(trees);
    [~, guess] = max(p_rf_sum);
    test_point(n, 3) = guess;
    disp(['Prediction is class ', num2str(guess)]);
end

figure;
plot_toydata(data_train);
plot_toydata(test_point);


%%
% 50x50 points
n=50;
data = zeros(n^2, 3);
r = [-1.5 1.5];
for i=1:n
    for j=1:n
        data((i-1)*n + j, 1) = r(1) + j*((r(2) - r(1)) / n);
        data((i-1)*n + j, 2) = r(1) + i*((r(2) - r(1)) / n);
    end
end

data = data_test;

for n=1:length(data)
    leaves = testTrees([data(n,1:2) 0], trees);
    p_rf = trees(1).prob(leaves,:);
    p_rf_sum = sum(p_rf)/length(trees);
    [~, guess] = max(p_rf_sum);
    data(n, 3) = guess;
%     disp(['Prediction is class ', num2str(guess)]);
end

figure;
% plot_toydata(data_train);
plot_toydata(data);

% leaves = testTrees(data_test, trees);
% % average the class distributions of leaf nodes of all trees
% p_rf = trees(1).prob(leaves,:);
% p_rf_sum = sum(p_rf)/length(trees)
% visualise(data_test,p_rf(1:22801,:),0,false);
% visualise(data_train, tree, 0, false);
% figure(fig);

% rootNode = struct('idx', idx, 't', nan, 'dim', -1, 'prob', []);
% rootNode = struct('idx',idx,'prob',[],'split_param',struct());
% [newRoot, rootL, rootR] = splitNode(data_train, rootNode, param);

